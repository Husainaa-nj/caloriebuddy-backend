[2m2025-08-29T08:05:13.329401Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-29T08:05:13.329684Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-29T08:05:13.334132Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-29T08:05:13.967633Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-29T08:05:49.227785Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-29T08:05:49.228568Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-29T08:05:49.228911Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-29T08:05:49.229580Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-29T08:05:49.229794Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-29T08:09:03.372532Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-29T08:09:03.372937Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-29T08:09:03.374664Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-29T08:09:03.845603Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-29T08:09:13.407327Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-29T08:09:13.408188Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-29T08:09:13.408423Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-29T08:09:13.408646Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-29T08:09:13.408832Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-29T08:09:53.858478Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:09:53.866706Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:09:53.870916Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:09:57.810029Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:09:57.812158Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:09:57.814016Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:10:03.067173Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:10:03.071899Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:10:03.074950Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:10:03.082153Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Preamble generation failed[0m
[2m2025-08-29T08:10:03.084663Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 496, in generate_preamble
    return await self._do_generate_preamble(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 633, in _do_generate_preamble
    canrep = await self._canrep_fluid_preamble_generator.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        prompt=prompt_builder, hints={"temperature": 0.1}
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 135, in wrapped_func
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 65, in apply
    raise e
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 125, in wrapped_func
    return await func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:10:03.144647Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process] Processing context for session L0N5itjuqM failed[0m
[2m2025-08-29T08:10:03.149351Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 173, in process
    await self._do_process(loaded_context)
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 272, in _do_process
    iteration_result = await self._run_preparation_iteration(context, preamble_task)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 452, in _run_preparation_iteration
    result = await self._run_initial_preparation_iteration(context, preamble_task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 554, in _run_initial_preparation_iteration
    if not await preamble_task:
           ^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 828, in preamble_task
    if await self._generate_preamble(context):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 859, in _generate_preamble
    for event_generation_result in await self._get_message_composer(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        context.agent
        ^^^^^^^^^^^^^
    ).generate_preamble(context=context):
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 496, in generate_preamble
    return await self._do_generate_preamble(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 633, in _do_generate_preamble
    canrep = await self._canrep_fluid_preamble_generator.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        prompt=prompt_builder, hints={"temperature": 0.1}
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 135, in wrapped_func
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 65, in apply
    raise e
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 125, in wrapped_func
    return await func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:10:03.153823Z[0m [[31m[1merror    [0m] [1m[Rkq3zB2LxWz::process] Processing error: ['Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 173, in process\n'
 '    await self._do_process(loaded_context)\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 272, in _do_process\n'
 '    iteration_result = await self._run_preparation_iteration(context, '
 'preamble_task)\n'
 '                       '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 452, in _run_preparation_iteration\n'
 '    result = await self._run_initial_preparation_iteration(context, '
 'preamble_task)\n'
 '             '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 554, in _run_initial_preparation_iteration\n'
 '    if not await preamble_task:\n'
 '           ^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 828, in preamble_task\n'
 '    if await self._generate_preamble(context):\n'
 '       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 859, in _generate_preamble\n'
 '    for event_generation_result in await self._get_message_composer(\n'
 '                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '        context.agent\n'
 '        ^^^^^^^^^^^^^\n'
 '    ).generate_preamble(context=context):\n'
 '    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 496, in generate_preamble\n'
 '    return await self._do_generate_preamble(context)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 633, in _do_generate_preamble\n'
 '    canrep = await self._canrep_fluid_preamble_generator.generate(\n'
 '             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '        prompt=prompt_builder, hints={"temperature": 0.1}\n'
 '        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 135, in wrapped_func\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 144, in wrapped_func\n'
 '    return await policy.apply(state, func, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 65, in apply\n'
 '    raise e\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 60, in apply\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 144, in wrapped_func\n'
 '    return await policy.apply(state, func, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 60, in apply\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 125, in wrapped_func\n'
 '    return await func(*args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\adapters\\nlp\\openai_service.py", '
 'line 137, in generate\n'
 '    return await self._do_generate(prompt, hints)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\adapters\\nlp\\openai_service.py", '
 'line 193, in _do_generate\n'
 '    response = await self._client.chat.completions.create(\n'
 '               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<4 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py", '
 'line 2583, in create\n'
 '    return await self._post(\n'
 '           ^^^^^^^^^^^^^^^^^\n'
 '    ...<48 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\_base_client.py", '
 'line 1794, in post\n'
 '    return await self.request(cast_to, opts, stream=stream, '
 'stream_cls=stream_cls)\n'
 '           '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\_base_client.py", '
 'line 1594, in request\n'
 '    raise self._make_status_error_from_response(err.response) from None\n',
 "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded "
 'your current quota, please check your plan and billing details. For more '
 'information on this error, read the docs: '
 "https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': "
 "'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"][0m
[2m2025-08-29T08:21:43.363379Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-29T08:21:43.363713Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-29T08:21:43.365368Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-29T08:21:43.675446Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-29T08:21:52.230426Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-29T08:21:52.230925Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-29T08:21:52.231164Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-29T08:21:52.231336Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-29T08:21:52.231573Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-29T08:21:52.515594Z[0m [[32m[1minfo     [0m] [1m[RIu0ofJr7b7] Session not found (id='L0N5itjuqM')[0m
[2m2025-08-29T08:22:15.563238Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:22:15.567237Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:22:15.568762Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:22:18.720514Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:22:18.723822Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:22:18.725191Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:22:22.949301Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:22:22.950983Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:22:22.952279Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:22:22.956459Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Preamble generation failed[0m
[2m2025-08-29T08:22:22.958613Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 496, in generate_preamble
    return await self._do_generate_preamble(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 633, in _do_generate_preamble
    canrep = await self._canrep_fluid_preamble_generator.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        prompt=prompt_builder, hints={"temperature": 0.1}
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 135, in wrapped_func
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 65, in apply
    raise e
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 125, in wrapped_func
    return await func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:22:22.979220Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process] Processing context for session SekTLVSUXo failed[0m
[2m2025-08-29T08:22:22.981623Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 173, in process
    await self._do_process(loaded_context)
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 272, in _do_process
    iteration_result = await self._run_preparation_iteration(context, preamble_task)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 452, in _run_preparation_iteration
    result = await self._run_initial_preparation_iteration(context, preamble_task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 554, in _run_initial_preparation_iteration
    if not await preamble_task:
           ^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 828, in preamble_task
    if await self._generate_preamble(context):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 859, in _generate_preamble
    for event_generation_result in await self._get_message_composer(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        context.agent
        ^^^^^^^^^^^^^
    ).generate_preamble(context=context):
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 496, in generate_preamble
    return await self._do_generate_preamble(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 633, in _do_generate_preamble
    canrep = await self._canrep_fluid_preamble_generator.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        prompt=prompt_builder, hints={"temperature": 0.1}
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 135, in wrapped_func
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 65, in apply
    raise e
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 125, in wrapped_func
    return await func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:22:22.985681Z[0m [[31m[1merror    [0m] [1m[ROF2PrUjP6W::process] Processing error: ['Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 173, in process\n'
 '    await self._do_process(loaded_context)\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 272, in _do_process\n'
 '    iteration_result = await self._run_preparation_iteration(context, '
 'preamble_task)\n'
 '                       '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 452, in _run_preparation_iteration\n'
 '    result = await self._run_initial_preparation_iteration(context, '
 'preamble_task)\n'
 '             '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 554, in _run_initial_preparation_iteration\n'
 '    if not await preamble_task:\n'
 '           ^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 828, in preamble_task\n'
 '    if await self._generate_preamble(context):\n'
 '       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 859, in _generate_preamble\n'
 '    for event_generation_result in await self._get_message_composer(\n'
 '                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '        context.agent\n'
 '        ^^^^^^^^^^^^^\n'
 '    ).generate_preamble(context=context):\n'
 '    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 496, in generate_preamble\n'
 '    return await self._do_generate_preamble(context)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 633, in _do_generate_preamble\n'
 '    canrep = await self._canrep_fluid_preamble_generator.generate(\n'
 '             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '        prompt=prompt_builder, hints={"temperature": 0.1}\n'
 '        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 135, in wrapped_func\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 144, in wrapped_func\n'
 '    return await policy.apply(state, func, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 65, in apply\n'
 '    raise e\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 60, in apply\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 144, in wrapped_func\n'
 '    return await policy.apply(state, func, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 60, in apply\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 125, in wrapped_func\n'
 '    return await func(*args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\adapters\\nlp\\openai_service.py", '
 'line 137, in generate\n'
 '    return await self._do_generate(prompt, hints)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\adapters\\nlp\\openai_service.py", '
 'line 193, in _do_generate\n'
 '    response = await self._client.chat.completions.create(\n'
 '               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<4 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py", '
 'line 2583, in create\n'
 '    return await self._post(\n'
 '           ^^^^^^^^^^^^^^^^^\n'
 '    ...<48 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\_base_client.py", '
 'line 1794, in post\n'
 '    return await self.request(cast_to, opts, stream=stream, '
 'stream_cls=stream_cls)\n'
 '           '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\_base_client.py", '
 'line 1594, in request\n'
 '    raise self._make_status_error_from_response(err.response) from None\n',
 "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded "
 'your current quota, please check your plan and billing details. For more '
 'information on this error, read the docs: '
 "https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': "
 "'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"][0m
[2m2025-08-29T08:30:18.095892Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-29T08:30:18.096259Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-29T08:30:18.097891Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-29T08:30:18.408407Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-29T08:30:26.885363Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-29T08:30:26.885806Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-29T08:30:26.886209Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-29T08:30:26.886786Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-29T08:30:26.887063Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-29T08:30:48.286549Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:30:48.293586Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:30:48.295702Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:30:51.800315Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:30:51.804377Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:30:51.807649Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:30:57.207063Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:30:57.210623Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:30:57.213833Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:30:57.217361Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Preamble generation failed[0m
[2m2025-08-29T08:30:57.219418Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 496, in generate_preamble
    return await self._do_generate_preamble(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 633, in _do_generate_preamble
    canrep = await self._canrep_fluid_preamble_generator.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        prompt=prompt_builder, hints={"temperature": 0.1}
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 135, in wrapped_func
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 65, in apply
    raise e
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 125, in wrapped_func
    return await func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:30:57.243012Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process] Processing context for session b8wUlCAxzF failed[0m
[2m2025-08-29T08:30:57.245200Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 173, in process
    await self._do_process(loaded_context)
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 272, in _do_process
    iteration_result = await self._run_preparation_iteration(context, preamble_task)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 452, in _run_preparation_iteration
    result = await self._run_initial_preparation_iteration(context, preamble_task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 554, in _run_initial_preparation_iteration
    if not await preamble_task:
           ^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 828, in preamble_task
    if await self._generate_preamble(context):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 859, in _generate_preamble
    for event_generation_result in await self._get_message_composer(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        context.agent
        ^^^^^^^^^^^^^
    ).generate_preamble(context=context):
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 496, in generate_preamble
    return await self._do_generate_preamble(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 633, in _do_generate_preamble
    canrep = await self._canrep_fluid_preamble_generator.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        prompt=prompt_builder, hints={"temperature": 0.1}
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 135, in wrapped_func
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 65, in apply
    raise e
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 125, in wrapped_func
    return await func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:30:57.249240Z[0m [[31m[1merror    [0m] [1m[RaQpjpcpnCK::process] Processing error: ['Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 173, in process\n'
 '    await self._do_process(loaded_context)\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 272, in _do_process\n'
 '    iteration_result = await self._run_preparation_iteration(context, '
 'preamble_task)\n'
 '                       '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 452, in _run_preparation_iteration\n'
 '    result = await self._run_initial_preparation_iteration(context, '
 'preamble_task)\n'
 '             '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 554, in _run_initial_preparation_iteration\n'
 '    if not await preamble_task:\n'
 '           ^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 828, in preamble_task\n'
 '    if await self._generate_preamble(context):\n'
 '       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 859, in _generate_preamble\n'
 '    for event_generation_result in await self._get_message_composer(\n'
 '                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '        context.agent\n'
 '        ^^^^^^^^^^^^^\n'
 '    ).generate_preamble(context=context):\n'
 '    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 496, in generate_preamble\n'
 '    return await self._do_generate_preamble(context)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 633, in _do_generate_preamble\n'
 '    canrep = await self._canrep_fluid_preamble_generator.generate(\n'
 '             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '        prompt=prompt_builder, hints={"temperature": 0.1}\n'
 '        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 135, in wrapped_func\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 144, in wrapped_func\n'
 '    return await policy.apply(state, func, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 65, in apply\n'
 '    raise e\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 60, in apply\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 144, in wrapped_func\n'
 '    return await policy.apply(state, func, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 60, in apply\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 125, in wrapped_func\n'
 '    return await func(*args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\adapters\\nlp\\openai_service.py", '
 'line 137, in generate\n'
 '    return await self._do_generate(prompt, hints)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\adapters\\nlp\\openai_service.py", '
 'line 193, in _do_generate\n'
 '    response = await self._client.chat.completions.create(\n'
 '               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<4 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py", '
 'line 2583, in create\n'
 '    return await self._post(\n'
 '           ^^^^^^^^^^^^^^^^^\n'
 '    ...<48 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\_base_client.py", '
 'line 1794, in post\n'
 '    return await self.request(cast_to, opts, stream=stream, '
 'stream_cls=stream_cls)\n'
 '           '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\_base_client.py", '
 'line 1594, in request\n'
 '    raise self._make_status_error_from_response(err.response) from None\n',
 "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded "
 'your current quota, please check your plan and billing details. For more '
 'information on this error, read the docs: '
 "https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': "
 "'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"][0m
[2m2025-08-29T08:33:33.966572Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-29T08:33:33.966825Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-29T08:33:33.968939Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-29T08:33:34.264021Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-29T08:36:15.345139Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-29T08:36:15.345379Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-29T08:36:15.346990Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-29T08:36:15.671015Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-29T08:36:23.988528Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-29T08:36:23.988999Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-29T08:36:23.989124Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-29T08:36:23.989486Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-29T08:36:23.989620Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-29T08:36:24.092687Z[0m [[32m[1minfo     [0m] [1m[RuKx0ktZr19] Session not found (id='b8wUlCAxzF')[0m
[2m2025-08-29T08:36:39.873396Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:36:39.879011Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:36:39.881128Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:36:43.648124Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:36:43.651459Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:36:43.654184Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:36:47.942393Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] OpenAI API rate limit exceeded. Possible reasons:
1. Your account may have insufficient API credits.
2. You may be using a free-tier account with limited request capacity.
3. You might have exceeded the requests-per-minute limit for your account.

Recommended actions:
- Check your OpenAI account balance and billing status.
- Review your API usage limits in OpenAI's dashboard.
- For more details on rate limits and usage tiers, visit:
  https://platform.openai.com/docs/guides/rate-limits/usage-tiers
[0m
[2m2025-08-29T08:36:47.945745Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] LLM Request (CannedResponsePreambleSchema) failed[0m
[2m2025-08-29T08:36:47.948117Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 275, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:36:47.951894Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Preamble generation failed[0m
[2m2025-08-29T08:36:47.954205Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process][MessageEventComposer][CannedResponseGenerator][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)][OpenAISchematicGenerator][LLM Request (CannedResponsePreambleSchema)] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 496, in generate_preamble
    return await self._do_generate_preamble(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 633, in _do_generate_preamble
    canrep = await self._canrep_fluid_preamble_generator.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        prompt=prompt_builder, hints={"temperature": 0.1}
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 135, in wrapped_func
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 65, in apply
    raise e
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 125, in wrapped_func
    return await func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:36:47.998052Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process] Processing context for session XIj9Gik5LK failed[0m
[2m2025-08-29T08:36:48.000964Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 173, in process
    await self._do_process(loaded_context)
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 272, in _do_process
    iteration_result = await self._run_preparation_iteration(context, preamble_task)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 452, in _run_preparation_iteration
    result = await self._run_initial_preparation_iteration(context, preamble_task)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 554, in _run_initial_preparation_iteration
    if not await preamble_task:
           ^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 828, in preamble_task
    if await self._generate_preamble(context):
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 859, in _generate_preamble
    for event_generation_result in await self._get_message_composer(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        context.agent
        ^^^^^^^^^^^^^
    ).generate_preamble(context=context):
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 496, in generate_preamble
    return await self._do_generate_preamble(context)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 633, in _do_generate_preamble
    canrep = await self._canrep_fluid_preamble_generator.generate(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        prompt=prompt_builder, hints={"temperature": 0.1}
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 135, in wrapped_func
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 65, in apply
    raise e
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 144, in wrapped_func
    return await policy.apply(state, func, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 60, in apply
    return await func(state, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\nlp\policies.py", line 125, in wrapped_func
    return await func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 137, in generate
    return await self._do_generate(prompt, hints)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\adapters\nlp\openai_service.py", line 193, in _do_generate
    response = await self._client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2583, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<48 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1794, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\openai\_base_client.py", line 1594, in request
    raise self._make_status_error_from_response(err.response) from None
 openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
[0m
[2m2025-08-29T08:36:48.004595Z[0m [[31m[1merror    [0m] [1m[RUjrCybq6AI::process] Processing error: ['Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 173, in process\n'
 '    await self._do_process(loaded_context)\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 272, in _do_process\n'
 '    iteration_result = await self._run_preparation_iteration(context, '
 'preamble_task)\n'
 '                       '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 452, in _run_preparation_iteration\n'
 '    result = await self._run_initial_preparation_iteration(context, '
 'preamble_task)\n'
 '             '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 554, in _run_initial_preparation_iteration\n'
 '    if not await preamble_task:\n'
 '           ^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 828, in preamble_task\n'
 '    if await self._generate_preamble(context):\n'
 '       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 859, in _generate_preamble\n'
 '    for event_generation_result in await self._get_message_composer(\n'
 '                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '        context.agent\n'
 '        ^^^^^^^^^^^^^\n'
 '    ).generate_preamble(context=context):\n'
 '    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 496, in generate_preamble\n'
 '    return await self._do_generate_preamble(context)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 633, in _do_generate_preamble\n'
 '    canrep = await self._canrep_fluid_preamble_generator.generate(\n'
 '             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '        prompt=prompt_builder, hints={"temperature": 0.1}\n'
 '        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 135, in wrapped_func\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 144, in wrapped_func\n'
 '    return await policy.apply(state, func, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 65, in apply\n'
 '    raise e\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 60, in apply\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 144, in wrapped_func\n'
 '    return await policy.apply(state, func, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 60, in apply\n'
 '    return await func(state, *args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\nlp\\policies.py", '
 'line 125, in wrapped_func\n'
 '    return await func(*args, **kwargs)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\adapters\\nlp\\openai_service.py", '
 'line 137, in generate\n'
 '    return await self._do_generate(prompt, hints)\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\adapters\\nlp\\openai_service.py", '
 'line 193, in _do_generate\n'
 '    response = await self._client.chat.completions.create(\n'
 '               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<4 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py", '
 'line 2583, in create\n'
 '    return await self._post(\n'
 '           ^^^^^^^^^^^^^^^^^\n'
 '    ...<48 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\_base_client.py", '
 'line 1794, in post\n'
 '    return await self.request(cast_to, opts, stream=stream, '
 'stream_cls=stream_cls)\n'
 '           '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\openai\\_base_client.py", '
 'line 1594, in request\n'
 '    raise self._make_status_error_from_response(err.response) from None\n',
 "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded "
 'your current quota, please check your plan and billing details. For more '
 'information on this error, read the docs: '
 "https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': "
 "'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"][0m
[2m2025-08-29T08:51:23.553377Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-29T08:51:23.553610Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-29T08:51:23.555235Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-29T08:51:23.861376Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-29T08:51:32.238983Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-29T08:51:32.239575Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-29T08:51:32.239888Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-29T08:51:32.240292Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-29T08:51:32.240394Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-29T08:51:32.396715Z[0m [[32m[1minfo     [0m] [1m[RF6zz1cBBFe] Session not found (id='XIj9Gik5LK')[0m
[2m2025-08-29T08:51:49.635111Z[0m [[32m[1minfo     [0m] [1m[RbHEgQuZpQR::process] Processing context for session rSYuGeambE finished in 6.644 seconds[0m
[2m2025-08-29T09:02:24.350342Z[0m [[32m[1minfo     [0m] [1m[RNcW8ISCfky::process] Processing context for session rSYuGeambE finished in 9.523 seconds[0m
[2m2025-08-29T11:02:55.009095Z[0m [[32m[1minfo     [0m] [1m[R36KW8OW4ea::process] Processing context for session rSYuGeambE finished in 9.435 seconds[0m
[2m2025-08-29T11:07:00.529907Z[0m [[32m[1minfo     [0m] [1m[RgzHFPyKxbz::process] Processing context for session rSYuGeambE finished in 5.249 seconds[0m
[2m2025-08-29T11:08:06.500412Z[0m [[32m[1minfo     [0m] [1m[RerHPe3SGrm::process] Processing context for session rSYuGeambE finished in 3.314 seconds[0m
[2m2025-08-30T07:38:35.213702Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-30T07:38:35.214046Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-30T07:38:35.215803Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-30T07:38:35.716769Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-30T07:38:44.310457Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-30T07:38:44.311020Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-30T07:38:44.311226Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-30T07:38:44.311484Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-30T07:38:44.311634Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-30T07:39:28.813679Z[0m [[32m[1minfo     [0m] [1m[RTwNECmd0ZD::process] Processing context for session oE773Ndt2V finished in 7.048 seconds[0m
[2m2025-08-30T08:02:38.989649Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-30T08:02:38.990054Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-30T08:02:38.991935Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-30T08:02:39.302298Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-30T08:02:47.634081Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-30T08:02:47.634590Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-30T08:02:47.634865Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-30T08:02:47.635050Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-30T08:02:47.635264Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-30T08:03:08.394177Z[0m [[32m[1minfo     [0m] [1m[RLl6iizp3hg::process] Processing context for session 90eHuuvzqI finished in 7.721 seconds[0m
[2m2025-08-30T08:18:10.031742Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-08-30T08:18:10.032097Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-08-30T08:18:10.033548Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-08-30T08:18:10.361466Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-08-30T08:18:19.546622Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-08-30T08:18:19.547032Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-08-30T08:18:19.547144Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-08-30T08:18:19.547334Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-08-30T08:18:19.547540Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-08-30T08:18:39.851951Z[0m [[32m[1minfo     [0m] [1m[RoVMm4WgpuY::process] Processing context for session ZAHKqkEkVH finished in 6.675 seconds[0m
[2m2025-09-01T08:08:56.246122Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-01T08:08:56.247380Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-01T08:08:56.249141Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-01T08:08:56.873558Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-01T08:09:05.857364Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-01T08:09:05.857885Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-01T08:09:05.858067Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-01T08:09:05.858194Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-01T08:09:05.858343Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-01T08:10:16.138567Z[0m [[32m[1minfo     [0m] [1m[RXlc4P6nvwe::process] Processing context for session hLiC3xqEQG finished in 7.753 seconds[0m
[2m2025-09-01T08:11:11.392340Z[0m [[32m[1minfo     [0m] [1m[Rq0NoLYiBgU::process] Processing context for session KkCPKBIbO9 finished in 6.945 seconds[0m
[2m2025-09-01T08:41:34.442633Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-01T08:41:34.443264Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-01T08:41:34.445762Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-01T08:41:34.750139Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-01T08:41:43.526242Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-01T08:41:43.526689Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-01T08:41:43.526921Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-01T08:41:43.527032Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-01T08:41:43.527143Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-01T08:41:43.662200Z[0m [[32m[1minfo     [0m] [1m[RjbV2EBcwoS] Session not found (id='KkCPKBIbO9')[0m
[2m2025-09-01T08:41:43.665830Z[0m [[32m[1minfo     [0m] [1m[R83QAicfwWI] Session not found (id='hLiC3xqEQG')[0m
[2m2025-09-01T08:54:27.441670Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-01T08:54:27.442208Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-01T08:54:27.444562Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-01T08:54:27.755586Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-01T08:54:36.708933Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-01T08:54:36.709454Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-01T08:54:36.709579Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-01T08:54:36.709659Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-01T08:54:36.709826Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-01T08:55:30.880154Z[0m [[32m[1minfo     [0m] [1m[RPtK01J1kfQ::process] Processing context for session xpCqYfWWs7 finished in 6.927 seconds[0m
[2m2025-09-01T09:42:00.065752Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-01T09:42:00.066720Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-01T09:42:00.071828Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-01T09:42:00.458509Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-01T09:42:09.400996Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-01T09:42:09.401375Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-01T09:42:09.401489Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-01T09:42:09.401568Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-01T09:42:09.401661Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-01T09:43:08.727434Z[0m [[32m[1minfo     [0m] [1m[R49bzQE6Xo3::process] Processing context for session R0PEyNEesG finished in 8.552 seconds[0m
[2m2025-09-01T09:47:46.089604Z[0m [[32m[1minfo     [0m] [1m[RHYoDqPdol8::process] Processing context for session R0PEyNEesG finished in 8.951 seconds[0m
[2m2025-09-01T15:32:44.813198Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-01T15:32:44.813510Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-01T15:32:44.815703Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-01T15:32:45.193034Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-01T15:32:54.569772Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-01T15:32:54.570250Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-01T15:32:54.570393Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-01T15:32:54.570629Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-01T15:32:54.570788Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-01T15:52:30.823466Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-01T15:52:30.823832Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-01T15:52:30.825832Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-01T15:52:31.163436Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-01T15:52:40.140894Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-01T15:52:40.141304Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-01T15:52:40.141421Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-01T15:52:40.141546Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-01T15:52:40.141731Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-01T15:53:00.735023Z[0m [[32m[1minfo     [0m] [1m[RYEjAAW1TBq::process] Processing context for session 8xCN2DHf2D finished in 7.051 seconds[0m
[2m2025-09-01T16:05:01.495489Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-01T16:05:01.495797Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-01T16:05:01.497306Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-01T16:05:01.802221Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-01T16:05:10.551619Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-01T16:05:10.552228Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-01T16:05:10.552376Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-01T16:05:10.552563Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-01T16:05:10.552774Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-01T16:05:10.557864Z[0m [[32m[1minfo     [0m] [1m[RSszZdZvDKx] Session not found (id='8xCN2DHf2D')[0m
[2m2025-09-01T16:05:32.834347Z[0m [[32m[1minfo     [0m] [1m[RIYGtj8J92c::process] Processing context for session LvYLLeb2MO finished in 8.545 seconds[0m
[2m2025-09-01T16:10:30.284064Z[0m [[32m[1minfo     [0m] [1m[Rs1B3k38qKu::process] Processing context for session we0hTXLopt finished in 6.316 seconds[0m
[2m2025-09-02T06:04:11.784199Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-02T06:04:11.784878Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-02T06:04:11.787653Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-02T06:04:12.676786Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-02T06:04:23.405380Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-02T06:04:23.406228Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-02T06:04:23.406578Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-02T06:04:23.407162Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-02T06:04:23.407456Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-02T07:57:00.998638Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-02T07:57:00.999212Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-02T07:57:01.001420Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-02T07:57:01.470072Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-02T07:57:11.463116Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-02T07:57:11.463763Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-02T07:57:11.463933Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-02T07:57:11.464093Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-02T07:57:11.464310Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-02T07:57:38.192685Z[0m [[32m[1minfo     [0m] [1m[R37D7WHqF98::process] Processing context for session QeqGgPUG6D finished in 7.857 seconds[0m
[2m2025-09-02T07:57:47.272383Z[0m [[32m[1minfo     [0m] [1m[RCirCQDHHSj] Item '7dAnQ4p6Q1' not found[0m
[2m2025-09-02T08:10:59.877244Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-02T08:10:59.877620Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-02T08:10:59.880214Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-02T08:11:00.362546Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-02T08:11:10.529613Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-02T08:11:10.530279Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-02T08:11:10.530695Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-02T08:11:10.530907Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-02T08:11:10.531084Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-02T08:12:44.034502Z[0m [[32m[1minfo     [0m] [1m[RVE918pVtjI::process] Processing context for session AjOnoxEeqC finished in 20.445 seconds[0m
[2m2025-09-02T08:13:48.832899Z[0m [[32m[1minfo     [0m] [1m[REQv1scVmqi::process] Processing context for session AjOnoxEeqC finished in 11.401 seconds[0m
[2m2025-09-02T14:28:32.493538Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-02T14:28:32.494037Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-02T14:28:32.496963Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-02T14:28:33.412544Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-02T14:28:44.663608Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Cancelling all remaining tasks (1)[0m
[2m2025-09-02T14:28:44.664369Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Shutting down[0m
[2m2025-09-02T14:28:44.664614Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Waiting for task 'websocket-logger' to finish[0m
[2m2025-09-02T14:35:04.872401Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-02T14:35:04.872773Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-02T14:35:04.875232Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-02T14:35:05.297462Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-02T14:35:15.801674Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Cancelling all remaining tasks (1)[0m
[2m2025-09-02T14:35:15.802450Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Shutting down[0m
[2m2025-09-02T14:35:15.802789Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Waiting for task 'websocket-logger' to finish[0m
[2m2025-09-02T15:38:03.691438Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-02T15:38:03.691858Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-02T15:38:03.693566Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-02T15:38:04.095938Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-02T15:38:14.590657Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-02T15:38:14.591234Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-02T15:38:14.591533Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-02T15:38:14.591800Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-02T15:38:14.592067Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-02T15:43:34.017011Z[0m [[32m[1minfo     [0m] [1m[R5fpC3pYszY::process] Processing context for session KT65WvgUX0 finished in 24.051 seconds[0m
[2m2025-09-05T14:56:19.871342Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-05T14:56:19.871986Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-05T14:56:19.873840Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-05T14:56:19.877566Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-05T14:56:28.585944Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Cancelling all remaining tasks (1)[0m
[2m2025-09-05T14:56:28.586359Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Shutting down[0m
[2m2025-09-05T14:56:28.586538Z[0m [[32m[1minfo     [0m] [1m[<main>] BackgroundTaskService: Waiting for task 'websocket-logger' to finish[0m
[2m2025-09-05T15:34:42.280114Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-05T15:34:42.280924Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-05T15:34:42.282966Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-05T15:34:42.285738Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-05T15:34:50.900352Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-05T15:34:50.900747Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-05T15:34:50.900865Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-05T15:34:50.900966Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-05T15:34:50.901074Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-05T15:35:14.821395Z[0m [[33m[1mwarning  [0m] [1m[R5eoVg0bM0b::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 0 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:35:14.825376Z[0m [[33m[1mwarning  [0m] [1m[R5eoVg0bM0b::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 1 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:35:14.828878Z[0m [[33m[1mwarning  [0m] [1m[R5eoVg0bM0b::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 2 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:35:14.832913Z[0m [[31m[1merror    [0m] [1m[R5eoVg0bM0b::process][MessageEventComposer][CannedResponseGenerator] Response generation failed[0m
[2m2025-09-05T15:35:14.834900Z[0m [[31m[1merror    [0m] [1m[R5eoVg0bM0b::process][MessageEventComposer][CannedResponseGenerator] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 813, in _do_generate_events
    generation_info, result = await self._generate_response(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1399, in _generate_response
    draft_prompt = self._build_draft_prompt(
        agent=context.agent,
    ...<11 lines>...
        shots=await self.shots(context.agent.composition_mode),
    )
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1158, in _build_draft_prompt
    builder.add_staged_tool_events(staged_tool_events)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 359, in add_staged_tool_events
    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL
    ~~~~~~~~~~~~~~~~^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 172, in adapt_event
    return json.dumps(
           ~~~~~~~~~~^
        {
        ^
    ...<3 lines>...
        }
        ^
    )
    ^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 261, in iterencode
    return _iterencode(o, 0)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
                    f'is not JSON serializable')
 TypeError: Object of type Document is not JSON serializable
 
The above exception was the direct cause of the following exception:

 Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 678, in generate_response
    return await self._do_generate_events(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 906, in _do_generate_events
    raise MessageCompositionError() from last_generation_exception
 parlant.core.engines.alpha.message_event_composer.MessageCompositionError: Message composition failed
[0m
[2m2025-09-05T15:35:14.870427Z[0m [[31m[1merror    [0m] [1m[R5eoVg0bM0b::process][MessageEventComposer][CannedResponseGenerator] Processing context for session dLiVSezN96 failed[0m
[2m2025-09-05T15:35:14.872568Z[0m [[31m[1merror    [0m] [1m[R5eoVg0bM0b::process][MessageEventComposer][CannedResponseGenerator] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 813, in _do_generate_events
    generation_info, result = await self._generate_response(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1399, in _generate_response
    draft_prompt = self._build_draft_prompt(
        agent=context.agent,
    ...<11 lines>...
        shots=await self.shots(context.agent.composition_mode),
    )
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1158, in _build_draft_prompt
    builder.add_staged_tool_events(staged_tool_events)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 359, in add_staged_tool_events
    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL
    ~~~~~~~~~~~~~~~~^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 172, in adapt_event
    return json.dumps(
           ~~~~~~~~~~^
        {
        ^
    ...<3 lines>...
        }
        ^
    )
    ^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 261, in iterencode
    return _iterencode(o, 0)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
                    f'is not JSON serializable')
 TypeError: Object of type Document is not JSON serializable
 
The above exception was the direct cause of the following exception:

 Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 173, in process
    await self._do_process(loaded_context)
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 305, in _do_process
    message_generation_inspections = await self._generate_messages(context, latch)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 874, in _generate_messages
    for event_generation_result in await self._get_message_composer(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    ):
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 678, in generate_response
    return await self._do_generate_events(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 906, in _do_generate_events
    raise MessageCompositionError() from last_generation_exception
 parlant.core.engines.alpha.message_event_composer.MessageCompositionError: Message composition failed
[0m
[2m2025-09-05T15:35:14.875713Z[0m [[31m[1merror    [0m] [1m[R5eoVg0bM0b::process][MessageEventComposer][CannedResponseGenerator] Processing error: ['Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 813, in _do_generate_events\n'
 '    generation_info, result = await self._generate_response(\n'
 '                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<5 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 1399, in _generate_response\n'
 '    draft_prompt = self._build_draft_prompt(\n'
 '        agent=context.agent,\n'
 '    ...<11 lines>...\n'
 '        shots=await self.shots(context.agent.composition_mode),\n'
 '    )\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 1158, in _build_draft_prompt\n'
 '    builder.add_staged_tool_events(staged_tool_events)\n'
 '    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", '
 'line 359, in add_staged_tool_events\n'
 '    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n'
 '    ~~~~~~~~~~~~~~~~^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", '
 'line 172, in adapt_event\n'
 '    return json.dumps(\n'
 '           ~~~~~~~~~~^\n'
 '        {\n'
 '        ^\n'
 '    ...<3 lines>...\n'
 '        }\n'
 '        ^\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", '
 'line 231, in dumps\n'
 '    return _default_encoder.encode(obj)\n'
 '           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 200, in encode\n'
 '    chunks = self.iterencode(o, _one_shot=True)\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 261, in iterencode\n'
 '    return _iterencode(o, 0)\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 180, in default\n'
 "    raise TypeError(f'Object of type {o.__class__.__name__} '\n"
 "                    f'is not JSON serializable')\n",
 'TypeError: Object of type Document is not JSON serializable\n',
 '\nThe above exception was the direct cause of the following exception:\n\n',
 'Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 173, in process\n'
 '    await self._do_process(loaded_context)\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 305, in _do_process\n'
 '    message_generation_inspections = await self._generate_messages(context, '
 'latch)\n'
 '                                     '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 874, in _generate_messages\n'
 '    for event_generation_result in await self._get_message_composer(\n'
 '                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<4 lines>...\n'
 '    ):\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 678, in generate_response\n'
 '    return await self._do_generate_events(\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<2 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 906, in _do_generate_events\n'
 '    raise MessageCompositionError() from last_generation_exception\n',
 'parlant.core.engines.alpha.message_event_composer.MessageCompositionError: '
 'Message composition failed\n'][0m
[2m2025-09-05T15:40:51.710439Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-05T15:40:51.710883Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-05T15:40:51.712887Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-05T15:40:51.715450Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-05T15:41:00.342330Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-05T15:41:00.342808Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-05T15:41:00.343008Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-05T15:41:00.343163Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-05T15:41:00.343258Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-05T15:41:00.430707Z[0m [[32m[1minfo     [0m] [1m[Rg0ppknjFUS] Session not found (id='dLiVSezN96')[0m
[2m2025-09-05T15:42:06.729812Z[0m [[33m[1mwarning  [0m] [1m[RtUvjF6LwLf::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 0 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:42:06.733958Z[0m [[33m[1mwarning  [0m] [1m[RtUvjF6LwLf::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 1 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:42:06.737099Z[0m [[33m[1mwarning  [0m] [1m[RtUvjF6LwLf::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 2 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:42:06.740572Z[0m [[31m[1merror    [0m] [1m[RtUvjF6LwLf::process][MessageEventComposer][CannedResponseGenerator] Response generation failed[0m
[2m2025-09-05T15:42:06.742689Z[0m [[31m[1merror    [0m] [1m[RtUvjF6LwLf::process][MessageEventComposer][CannedResponseGenerator] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 813, in _do_generate_events
    generation_info, result = await self._generate_response(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1399, in _generate_response
    draft_prompt = self._build_draft_prompt(
        agent=context.agent,
    ...<11 lines>...
        shots=await self.shots(context.agent.composition_mode),
    )
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1158, in _build_draft_prompt
    builder.add_staged_tool_events(staged_tool_events)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 359, in add_staged_tool_events
    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL
    ~~~~~~~~~~~~~~~~^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 172, in adapt_event
    return json.dumps(
           ~~~~~~~~~~^
        {
        ^
    ...<3 lines>...
        }
        ^
    )
    ^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 261, in iterencode
    return _iterencode(o, 0)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
                    f'is not JSON serializable')
 TypeError: Object of type Document is not JSON serializable
 
The above exception was the direct cause of the following exception:

 Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 678, in generate_response
    return await self._do_generate_events(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 906, in _do_generate_events
    raise MessageCompositionError() from last_generation_exception
 parlant.core.engines.alpha.message_event_composer.MessageCompositionError: Message composition failed
[0m
[2m2025-09-05T15:42:06.750187Z[0m [[31m[1merror    [0m] [1m[RtUvjF6LwLf::process][MessageEventComposer][CannedResponseGenerator] Processing context for session YaxMM1jBV8 failed[0m
[2m2025-09-05T15:42:06.752528Z[0m [[31m[1merror    [0m] [1m[RtUvjF6LwLf::process][MessageEventComposer][CannedResponseGenerator] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 813, in _do_generate_events
    generation_info, result = await self._generate_response(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1399, in _generate_response
    draft_prompt = self._build_draft_prompt(
        agent=context.agent,
    ...<11 lines>...
        shots=await self.shots(context.agent.composition_mode),
    )
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1158, in _build_draft_prompt
    builder.add_staged_tool_events(staged_tool_events)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 359, in add_staged_tool_events
    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL
    ~~~~~~~~~~~~~~~~^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 172, in adapt_event
    return json.dumps(
           ~~~~~~~~~~^
        {
        ^
    ...<3 lines>...
        }
        ^
    )
    ^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 261, in iterencode
    return _iterencode(o, 0)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
                    f'is not JSON serializable')
 TypeError: Object of type Document is not JSON serializable
 
The above exception was the direct cause of the following exception:

 Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 173, in process
    await self._do_process(loaded_context)
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 305, in _do_process
    message_generation_inspections = await self._generate_messages(context, latch)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 874, in _generate_messages
    for event_generation_result in await self._get_message_composer(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    ):
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 678, in generate_response
    return await self._do_generate_events(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 906, in _do_generate_events
    raise MessageCompositionError() from last_generation_exception
 parlant.core.engines.alpha.message_event_composer.MessageCompositionError: Message composition failed
[0m
[2m2025-09-05T15:42:06.755417Z[0m [[31m[1merror    [0m] [1m[RtUvjF6LwLf::process][MessageEventComposer][CannedResponseGenerator] Processing error: ['Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 813, in _do_generate_events\n'
 '    generation_info, result = await self._generate_response(\n'
 '                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<5 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 1399, in _generate_response\n'
 '    draft_prompt = self._build_draft_prompt(\n'
 '        agent=context.agent,\n'
 '    ...<11 lines>...\n'
 '        shots=await self.shots(context.agent.composition_mode),\n'
 '    )\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 1158, in _build_draft_prompt\n'
 '    builder.add_staged_tool_events(staged_tool_events)\n'
 '    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", '
 'line 359, in add_staged_tool_events\n'
 '    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n'
 '    ~~~~~~~~~~~~~~~~^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", '
 'line 172, in adapt_event\n'
 '    return json.dumps(\n'
 '           ~~~~~~~~~~^\n'
 '        {\n'
 '        ^\n'
 '    ...<3 lines>...\n'
 '        }\n'
 '        ^\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", '
 'line 231, in dumps\n'
 '    return _default_encoder.encode(obj)\n'
 '           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 200, in encode\n'
 '    chunks = self.iterencode(o, _one_shot=True)\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 261, in iterencode\n'
 '    return _iterencode(o, 0)\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 180, in default\n'
 "    raise TypeError(f'Object of type {o.__class__.__name__} '\n"
 "                    f'is not JSON serializable')\n",
 'TypeError: Object of type Document is not JSON serializable\n',
 '\nThe above exception was the direct cause of the following exception:\n\n',
 'Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 173, in process\n'
 '    await self._do_process(loaded_context)\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 305, in _do_process\n'
 '    message_generation_inspections = await self._generate_messages(context, '
 'latch)\n'
 '                                     '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 874, in _generate_messages\n'
 '    for event_generation_result in await self._get_message_composer(\n'
 '                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<4 lines>...\n'
 '    ):\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 678, in generate_response\n'
 '    return await self._do_generate_events(\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<2 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 906, in _do_generate_events\n'
 '    raise MessageCompositionError() from last_generation_exception\n',
 'parlant.core.engines.alpha.message_event_composer.MessageCompositionError: '
 'Message composition failed\n'][0m
[2m2025-09-05T15:43:48.968381Z[0m [[33m[1mwarning  [0m] [1m[RdYvONJPCkW::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 0 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:43:48.970299Z[0m [[33m[1mwarning  [0m] [1m[RdYvONJPCkW::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 1 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:43:48.971919Z[0m [[33m[1mwarning  [0m] [1m[RdYvONJPCkW::process][MessageEventComposer][CannedResponseGenerator] Generation attempt 2 failed: ['Traceback (most recent call last):\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 813, in _do_generate_events\n    generation_info, result = await self._generate_response(\n                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n    ...<5 lines>...\n    )\n    ^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1399, in _generate_response\n    draft_prompt = self._build_draft_prompt(\n        agent=context.agent,\n    ...<11 lines>...\n        shots=await self.shots(context.agent.composition_mode),\n    )\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", line 1158, in _build_draft_prompt\n    builder.add_staged_tool_events(staged_tool_events)\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 359, in add_staged_tool_events\n    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n    ~~~~~~~~~~~~~~~~^^^\n', '  File "C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", line 172, in adapt_event\n    return json.dumps(\n           ~~~~~~~~~~^\n        {\n        ^\n    ...<3 lines>...\n        }\n        ^\n    )\n    ^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", line 231, in dumps\n    return _default_encoder.encode(obj)\n           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 200, in encode\n    chunks = self.iterencode(o, _one_shot=True)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 261, in iterencode\n    return _iterencode(o, 0)\n', '  File "C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", line 180, in default\n    raise TypeError(f\'Object of type {o.__class__.__name__} \'\n                    f\'is not JSON serializable\')\n', 'TypeError: Object of type Document is not JSON serializable\n'][0m
[2m2025-09-05T15:43:48.973771Z[0m [[31m[1merror    [0m] [1m[RdYvONJPCkW::process][MessageEventComposer][CannedResponseGenerator] Response generation failed[0m
[2m2025-09-05T15:43:48.975294Z[0m [[31m[1merror    [0m] [1m[RdYvONJPCkW::process][MessageEventComposer][CannedResponseGenerator] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 813, in _do_generate_events
    generation_info, result = await self._generate_response(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1399, in _generate_response
    draft_prompt = self._build_draft_prompt(
        agent=context.agent,
    ...<11 lines>...
        shots=await self.shots(context.agent.composition_mode),
    )
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1158, in _build_draft_prompt
    builder.add_staged_tool_events(staged_tool_events)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 359, in add_staged_tool_events
    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL
    ~~~~~~~~~~~~~~~~^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 172, in adapt_event
    return json.dumps(
           ~~~~~~~~~~^
        {
        ^
    ...<3 lines>...
        }
        ^
    )
    ^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 261, in iterencode
    return _iterencode(o, 0)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
                    f'is not JSON serializable')
 TypeError: Object of type Document is not JSON serializable
 
The above exception was the direct cause of the following exception:

 Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 678, in generate_response
    return await self._do_generate_events(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 906, in _do_generate_events
    raise MessageCompositionError() from last_generation_exception
 parlant.core.engines.alpha.message_event_composer.MessageCompositionError: Message composition failed
[0m
[2m2025-09-05T15:43:48.980428Z[0m [[31m[1merror    [0m] [1m[RdYvONJPCkW::process][MessageEventComposer][CannedResponseGenerator] Processing context for session YaxMM1jBV8 failed[0m
[2m2025-09-05T15:43:48.983319Z[0m [[31m[1merror    [0m] [1m[RdYvONJPCkW::process][MessageEventComposer][CannedResponseGenerator] Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 813, in _do_generate_events
    generation_info, result = await self._generate_response(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<5 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1399, in _generate_response
    draft_prompt = self._build_draft_prompt(
        agent=context.agent,
    ...<11 lines>...
        shots=await self.shots(context.agent.composition_mode),
    )
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 1158, in _build_draft_prompt
    builder.add_staged_tool_events(staged_tool_events)
    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 359, in add_staged_tool_events
    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL
    ~~~~~~~~~~~~~~~~^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\prompt_builder.py", line 172, in adapt_event
    return json.dumps(
           ~~~~~~~~~~^
        {
        ^
    ...<3 lines>...
        }
        ^
    )
    ^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\__init__.py", line 231, in dumps
    return _default_encoder.encode(obj)
           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 200, in encode
    chunks = self.iterencode(o, _one_shot=True)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 261, in iterencode
    return _iterencode(o, 0)
   File "C:\Users\husai\AppData\Local\Programs\Python\Python313\Lib\json\encoder.py", line 180, in default
    raise TypeError(f'Object of type {o.__class__.__name__} '
                    f'is not JSON serializable')
 TypeError: Object of type Document is not JSON serializable
 
The above exception was the direct cause of the following exception:

 Traceback (most recent call last):
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 277, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\loggers.py", line 411, in operation
    yield
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 173, in process
    await self._do_process(loaded_context)
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 305, in _do_process
    message_generation_inspections = await self._generate_messages(context, latch)
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\engine.py", line 874, in _generate_messages
    for event_generation_result in await self._get_message_composer(
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<4 lines>...
    ):
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 678, in generate_response
    return await self._do_generate_events(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
   File "C:\Users\husai\calorieBuddy\.venv\Lib\site-packages\parlant\core\engines\alpha\canned_response_generator.py", line 906, in _do_generate_events
    raise MessageCompositionError() from last_generation_exception
 parlant.core.engines.alpha.message_event_composer.MessageCompositionError: Message composition failed
[0m
[2m2025-09-05T15:43:48.985815Z[0m [[31m[1merror    [0m] [1m[RdYvONJPCkW::process][MessageEventComposer][CannedResponseGenerator] Processing error: ['Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 813, in _do_generate_events\n'
 '    generation_info, result = await self._generate_response(\n'
 '                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<5 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 1399, in _generate_response\n'
 '    draft_prompt = self._build_draft_prompt(\n'
 '        agent=context.agent,\n'
 '    ...<11 lines>...\n'
 '        shots=await self.shots(context.agent.composition_mode),\n'
 '    )\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 1158, in _build_draft_prompt\n'
 '    builder.add_staged_tool_events(staged_tool_events)\n'
 '    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", '
 'line 359, in add_staged_tool_events\n'
 '    self.adapt_event(e) for e in events if e.kind == EventKind.TOOL\n'
 '    ~~~~~~~~~~~~~~~~^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\prompt_builder.py", '
 'line 172, in adapt_event\n'
 '    return json.dumps(\n'
 '           ~~~~~~~~~~^\n'
 '        {\n'
 '        ^\n'
 '    ...<3 lines>...\n'
 '        }\n'
 '        ^\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py", '
 'line 231, in dumps\n'
 '    return _default_encoder.encode(obj)\n'
 '           ~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 200, in encode\n'
 '    chunks = self.iterencode(o, _one_shot=True)\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 261, in iterencode\n'
 '    return _iterencode(o, 0)\n',
 '  File '
 '"C:\\Users\\husai\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\encoder.py", '
 'line 180, in default\n'
 "    raise TypeError(f'Object of type {o.__class__.__name__} '\n"
 "                    f'is not JSON serializable')\n",
 'TypeError: Object of type Document is not JSON serializable\n',
 '\nThe above exception was the direct cause of the following exception:\n\n',
 'Traceback (most recent call last):\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 173, in process\n'
 '    await self._do_process(loaded_context)\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 305, in _do_process\n'
 '    message_generation_inspections = await self._generate_messages(context, '
 'latch)\n'
 '                                     '
 '^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\engine.py", '
 'line 874, in _generate_messages\n'
 '    for event_generation_result in await self._get_message_composer(\n'
 '                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<4 lines>...\n'
 '    ):\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 678, in generate_response\n'
 '    return await self._do_generate_events(\n'
 '           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n'
 '    ...<2 lines>...\n'
 '    )\n'
 '    ^\n',
 '  File '
 '"C:\\Users\\husai\\calorieBuddy\\.venv\\Lib\\site-packages\\parlant\\core\\engines\\alpha\\canned_response_generator.py", '
 'line 906, in _do_generate_events\n'
 '    raise MessageCompositionError() from last_generation_exception\n',
 'parlant.core.engines.alpha.message_event_composer.MessageCompositionError: '
 'Message composition failed\n'][0m
[2m2025-09-05T15:50:19.809329Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-05T15:50:19.809799Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-05T15:50:19.811729Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-05T15:50:19.814253Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-05T15:50:28.523122Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-05T15:50:28.523596Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-05T15:50:28.524233Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-05T15:50:28.524444Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-05T15:50:28.524643Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-05T15:50:52.984329Z[0m [[32m[1minfo     [0m] [1m[RlpgcYI24OU::process] Processing context for session s4BZ9LJZKv finished in 10.265 seconds[0m
[2m2025-09-05T15:51:15.912417Z[0m [[32m[1minfo     [0m] [1m[RjxjyErVqmM::process] Processing context for session s4BZ9LJZKv finished in 7.986 seconds[0m
[2m2025-09-05T15:52:03.070352Z[0m [[32m[1minfo     [0m] [1m[Rktvq8jlmrw::process] Processing context for session s4BZ9LJZKv finished in 12.461 seconds[0m
[2m2025-09-05T15:52:36.435336Z[0m [[32m[1minfo     [0m] [1m[RI6gBG4jSXT::process] Processing context for session s4BZ9LJZKv finished in 8.68 seconds[0m
[2m2025-09-05T15:55:16.187975Z[0m [[32m[1minfo     [0m] [1m[R4zwTNzmIzz::process] Processing context for session s4BZ9LJZKv finished in 8.794 seconds[0m
[2m2025-09-05T15:55:46.910818Z[0m [[32m[1minfo     [0m] [1m[RTfOJEAglaQ::process] Processing context for session s4BZ9LJZKv finished in 6.654 seconds[0m
[2m2025-09-11T10:27:58.873435Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-11T10:27:58.874215Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-11T10:27:58.877680Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-11T10:27:58.883611Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-11T10:28:08.865384Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-11T10:28:08.866045Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-11T10:28:08.866261Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-11T10:28:08.866432Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-11T10:28:08.866594Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-11T10:30:38.042635Z[0m [[32m[1minfo     [0m] [1m[RaQ7G44qnTy::process] Processing context for session 1ERRXjqySU finished in 10.618 seconds[0m
[2m2025-09-11T10:31:50.319609Z[0m [[32m[1minfo     [0m] [1m[R49GbMKo9hI::process] Processing context for session 1ERRXjqySU finished in 16.823 seconds[0m
[2m2025-09-11T10:32:31.644367Z[0m [[32m[1minfo     [0m] [1m[RdK4XG42hX5::process] Processing context for session 1ERRXjqySU finished in 10.149 seconds[0m
[2m2025-09-11T10:35:01.275865Z[0m [[32m[1minfo     [0m] [1m[RLVWggDeQz3::process] Processing context for session BDKOJNnQZk finished in 7.264 seconds[0m
[2m2025-09-11T10:35:50.815541Z[0m [[32m[1minfo     [0m] [1m[RH7RXvAINzu::process] Processing context for session BDKOJNnQZk finished in 22.068 seconds[0m
[2m2025-09-11T10:37:02.853464Z[0m [[32m[1minfo     [0m] [1m[Rrc9kdKFVIz::process] Processing context for session BDKOJNnQZk finished in 8.885 seconds[0m
[2m2025-09-11T10:37:54.277377Z[0m [[32m[1minfo     [0m] [1m[RRednuvd2re::process] Processing context for session BDKOJNnQZk finished in 9.08 seconds[0m
[2m2025-09-11T11:06:11.139840Z[0m [[32m[1minfo     [0m] [1m[RKmDYDiA1xy::process] Processing context for session tKI6Gsj2df finished in 10.078 seconds[0m
[2m2025-09-11T11:06:54.328474Z[0m [[32m[1minfo     [0m] [1m[RYEwCedktX5::process] Processing context for session tKI6Gsj2df finished in 8.242 seconds[0m
[2m2025-09-11T11:36:03.896559Z[0m [[32m[1minfo     [0m] [1m[RiLle98zEYN::process] Processing context for session BDKOJNnQZk finished in 4.896 seconds[0m
[2m2025-09-11T11:38:53.720557Z[0m [[32m[1minfo     [0m] [1m[RjS7oKEI4Ah::process] Processing context for session tKI6Gsj2df finished in 7.714 seconds[0m
[2m2025-09-11T11:39:55.831240Z[0m [[32m[1minfo     [0m] [1m[Rv1yEsKQAkd::process] Processing context for session tKI6Gsj2df finished in 16.141 seconds[0m
[2m2025-09-11T11:41:36.954278Z[0m [[32m[1minfo     [0m] [1m[RMufO59H6vw::process] Processing context for session tKI6Gsj2df finished in 24.649 seconds[0m
[2m2025-09-11T11:44:52.941727Z[0m [[32m[1minfo     [0m] [1m[RVqmwpCBnRk::process] Processing context for session tKI6Gsj2df finished in 11.837 seconds[0m
[2m2025-09-11T11:45:31.789712Z[0m [[32m[1minfo     [0m] [1m[RivfP6XQOTS::process] Processing context for session tKI6Gsj2df finished in 3.441 seconds[0m
[2m2025-09-11T11:46:08.185006Z[0m [[32m[1minfo     [0m] [1m[RaeiPerwwfY::process] Processing context for session tKI6Gsj2df finished in 3.37 seconds[0m
[2m2025-09-11T11:46:35.970597Z[0m [[32m[1minfo     [0m] [1m[RY5zbfXD1po::process] Processing context for session tKI6Gsj2df finished in 5.772 seconds[0m
[2m2025-09-11T11:47:27.017417Z[0m [[32m[1minfo     [0m] [1m[RqS4QS2hSBy::process] Processing context for session tKI6Gsj2df finished in 3.925 seconds[0m
[2m2025-09-11T11:48:00.659858Z[0m [[32m[1minfo     [0m] [1m[Rk6a0gUMFDA::process] Processing context for session tKI6Gsj2df finished in 4.985 seconds[0m
[2m2025-09-11T11:52:33.762247Z[0m [[32m[1minfo     [0m] [1m[RRgHK88SUiZ::process] Processing context for session tKI6Gsj2df finished in 3.784 seconds[0m
[2m2025-09-11T11:57:00.726969Z[0m [[32m[1minfo     [0m] [1m[R7ldOOleqlA::process] Processing context for session tKI6Gsj2df finished in 3.777 seconds[0m
[2m2025-09-11T11:58:09.763301Z[0m [[32m[1minfo     [0m] [1m[RngatyTVJGd::process] Processing context for session tKI6Gsj2df finished in 8.31 seconds[0m
[2m2025-09-11T12:05:35.264875Z[0m [[32m[1minfo     [0m] [1m[RdoRSqGY6lB::process] Processing context for session tKI6Gsj2df finished in 5.221 seconds[0m
[2m2025-09-11T12:06:51.969675Z[0m [[32m[1minfo     [0m] [1m[Rdm8xFxkXxd::process] Processing context for session tKI6Gsj2df finished in 9.669 seconds[0m
[2m2025-09-11T12:08:33.643137Z[0m [[32m[1minfo     [0m] [1m[RmaJnByRXTG::process] Processing context for session tKI6Gsj2df finished in 3.301 seconds[0m
[2m2025-09-11T12:08:56.240892Z[0m [[32m[1minfo     [0m] [1m[Rc2XXg6mS6J::process] Processing context for session tKI6Gsj2df finished in 3.032 seconds[0m
[2m2025-09-11T12:11:29.946757Z[0m [[32m[1minfo     [0m] [1m[RjtXLZPgiHJ::process] Processing context for session tKI6Gsj2df finished in 3.182 seconds[0m
[2m2025-09-11T12:13:01.528155Z[0m [[32m[1minfo     [0m] [1m[RUwbynp2JTV::process] Processing context for session tKI6Gsj2df finished in 3.11 seconds[0m
[2m2025-09-11T12:13:54.165079Z[0m [[32m[1minfo     [0m] [1m[RLslQZkkAlh::process] Processing context for session tKI6Gsj2df finished in 3.997 seconds[0m
[2m2025-09-11T12:18:31.009511Z[0m [[32m[1minfo     [0m] [1m[RRafRt5EvoZ::process] Processing context for session tKI6Gsj2df finished in 3.86 seconds[0m
[2m2025-09-11T12:20:22.693388Z[0m [[32m[1minfo     [0m] [1m[RqDPNubiaio::process] Processing context for session tKI6Gsj2df finished in 9.638 seconds[0m
[2m2025-09-11T12:21:33.798832Z[0m [[32m[1minfo     [0m] [1m[Rm1epMtcWUp::process] Processing context for session tKI6Gsj2df finished in 3.048 seconds[0m
[2m2025-09-11T12:24:23.198923Z[0m [[32m[1minfo     [0m] [1m[R5iTBmqsn3S::process] Processing context for session tKI6Gsj2df finished in 6.533 seconds[0m
[2m2025-09-11T12:28:47.892241Z[0m [[32m[1minfo     [0m] [1m[RzS4FsWcFlD::process] Processing context for session tKI6Gsj2df finished in 2.617 seconds[0m
[2m2025-09-11T12:30:26.343320Z[0m [[32m[1minfo     [0m] [1m[RxvdopjZY11::process] Processing context for session tKI6Gsj2df finished in 7.857 seconds[0m
[2m2025-09-11T12:30:41.691663Z[0m [[32m[1minfo     [0m] [1m[R9IL4Uh7YYE::process] Processing context for session tKI6Gsj2df finished in 5.542 seconds[0m
[2m2025-09-11T12:33:32.153294Z[0m [[32m[1minfo     [0m] [1m[RSVbh1OyphD::process] Processing context for session tKI6Gsj2df finished in 9.543 seconds[0m
[2m2025-09-11T12:38:33.138569Z[0m [[32m[1minfo     [0m] [1m[RzyrRHaQL8v::process] Processing context for session tKI6Gsj2df finished in 25.728 seconds[0m
[2m2025-09-13T10:46:10.255478Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-13T10:46:10.255851Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-13T10:46:10.258390Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-13T10:46:10.263923Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-13T10:46:19.610879Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-13T10:46:19.611543Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-13T10:46:19.611856Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-13T10:46:19.612134Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-13T10:46:19.612719Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-13T10:46:19.694702Z[0m [[32m[1minfo     [0m] [1m[RHWnHYVaI4p] Session not found (id='BDKOJNnQZk')[0m
[2m2025-09-13T10:46:44.945372Z[0m [[32m[1minfo     [0m] [1m[RlTV2dDuiVm::process] Processing context for session DwhOQ0Ej97 finished in 9.015 seconds[0m
[2m2025-09-13T10:47:45.550268Z[0m [[32m[1minfo     [0m] [1m[RU3FsH7dBFY::process] Processing context for session DwhOQ0Ej97 finished in 8.307 seconds[0m
[2m2025-09-13T10:48:21.123744Z[0m [[32m[1minfo     [0m] [1m[RvIFkcPpUwl::process] Processing context for session DwhOQ0Ej97 finished in 6.929 seconds[0m
[2m2025-09-13T10:49:28.782773Z[0m [[32m[1minfo     [0m] [1m[RXGah2EcyNA::process] Processing context for session DwhOQ0Ej97 finished in 13.094 seconds[0m
[2m2025-09-13T11:08:04.836998Z[0m [[32m[1minfo     [0m] [1m[Riup5BQiJoh::process] Processing context for session DwhOQ0Ej97 finished in 15.767 seconds[0m
[2m2025-09-15T17:08:41.753634Z[0m [[32m[1minfo     [0m] [1m[<main>] Parlant server version 3.0.2[0m
[2m2025-09-15T17:08:41.754109Z[0m [[32m[1minfo     [0m] [1m[<main>] Using home directory 'C:\Users\husai\calorieBuddy\parlant-data'[0m
[2m2025-09-15T17:08:41.757472Z[0m [[32m[1minfo     [0m] [1m[<main>] No external modules selected[0m
[2m2025-09-15T17:08:41.764321Z[0m [[32m[1minfo     [0m] [1m[<main>] Initialized OpenAIService[0m
[2m2025-09-15T17:08:51.365618Z[0m [[32m[1minfo     [0m] [1m[<main>] .-----------------------------------------.[0m
[2m2025-09-15T17:08:51.366357Z[0m [[32m[1minfo     [0m] [1m[<main>] | Server is ready for some serious action |[0m
[2m2025-09-15T17:08:51.366767Z[0m [[32m[1minfo     [0m] [1m[<main>] '-----------------------------------------'[0m
[2m2025-09-15T17:08:51.367267Z[0m [[32m[1minfo     [0m] [1m[<main>] Server authorization policy: development[0m
[2m2025-09-15T17:08:51.367556Z[0m [[32m[1minfo     [0m] [1m[<main>] Try the Sandbox UI at http://localhost:8800[0m
[2m2025-09-15T17:17:08.247406Z[0m [[32m[1minfo     [0m] [1m[RvSZJoDV7QA::process] Processing context for session 8ERuz4hW70 finished in 12.108 seconds[0m
[2m2025-09-15T17:17:48.674900Z[0m [[32m[1minfo     [0m] [1m[Rh9iL55SkJn::process] Processing context for session 8ERuz4hW70 finished in 7.953 seconds[0m
[2m2025-09-15T17:19:11.363314Z[0m [[32m[1minfo     [0m] [1m[Re7hxYksCJI::process] Processing context for session 8ERuz4hW70 finished in 15.947 seconds[0m
[2m2025-09-15T17:19:44.065076Z[0m [[32m[1minfo     [0m] [1m[RPHk1cgsGW7::process] Processing context for session 8ERuz4hW70 finished in 3.515 seconds[0m
